{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mauri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV file...\n",
      "Parsed 79062 sentences.\n",
      "Found 63006 unique words tokens.\n",
      "Using vocabulary size 8000.\n",
      "The least frequent word in our vocabulary is 'appointments' and appeared 10 times.\n",
      "\n",
      "Example sentence: 'SENTENCE_START i joined a new league this year and they have different scoring rules than i'm used to. SENTENCE_END'\n",
      "\n",
      "Example sentence after Pre-processing: '['SENTENCE_START', 'i', 'joined', 'a', 'new', 'league', 'this', 'year', 'and', 'they', 'have', 'different', 'scoring', 'rules', 'than', 'i', \"'m\", 'used', 'to', '.', 'SENTENCE_END']'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mauri\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\ipykernel_launcher.py:48: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (78489,)\n",
      "y_train shape: (78489,)\n",
      "x:\n",
      "SENTENCE_START what are n't you understanding about this ? !\n",
      "[0, 52, 28, 17, 10, 858, 55, 26, 35, 70]\n",
      "\n",
      "y:\n",
      "what are n't you understanding about this ? ! SENTENCE_END\n",
      "[52, 28, 17, 10, 858, 55, 26, 35, 70, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mauri\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\ipykernel_launcher.py:49: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import itertools\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def getSentenceData(path, vocabulary_size=8000):\n",
    "    unknown_token = \"UNKNOWN_TOKEN\"\n",
    "    sentence_start_token = \"SENTENCE_START\"\n",
    "    sentence_end_token = \"SENTENCE_END\"\n",
    "\n",
    "    # Read the data and append SENTENCE_START and SENTENCE_END tokens\n",
    "    print(\"Reading CSV file...\")\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f, skipinitialspace=True)\n",
    "        # Split full comments into sentences\n",
    "        sentences = itertools.chain(*[nltk.sent_tokenize(x[0].lower()) for x in reader])\n",
    "        # Append SENTENCE_START and SENTENCE_END\n",
    "        sentences = [\"%s %s %s\" % (sentence_start_token, x, sentence_end_token) for x in sentences]\n",
    "    print(\"Parsed %d sentences.\" % (len(sentences)))\n",
    "\n",
    "    # Tokenize the sentences into words\n",
    "    tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    # Filter the sentences having few words (including SENTENCE_START and SENTENCE_END)\n",
    "    tokenized_sentences = list(filter(lambda x: len(x) > 3, tokenized_sentences))\n",
    "\n",
    "    # Count the word frequencies\n",
    "    word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "    print(\"Found %d unique words tokens.\" % len(word_freq.items()))\n",
    "\n",
    "    # Get the most common words and build index_to_word and word_to_index vectors\n",
    "    vocab = word_freq.most_common(vocabulary_size-1)\n",
    "    index_to_word = [x[0] for x in vocab]\n",
    "    index_to_word.append(unknown_token)\n",
    "    word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n",
    "\n",
    "    print(\"Using vocabulary size %d.\" % vocabulary_size)\n",
    "    print(\"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1]))\n",
    "\n",
    "    # Replace all words not in our vocabulary with the unknown token\n",
    "    for i, sent in enumerate(tokenized_sentences):\n",
    "        tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]\n",
    "\n",
    "    print(\"\\nExample sentence: '%s'\" % sentences[1])\n",
    "    print(\"\\nExample sentence after Pre-processing: '%s'\\n\" % tokenized_sentences[0])\n",
    "\n",
    "    # Create the training data\n",
    "    X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
    "    y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])\n",
    "\n",
    "    print(\"X_train shape: \" + str(X_train.shape))\n",
    "    print(\"y_train shape: \" + str(y_train.shape))\n",
    "\n",
    "    # Print an training data example\n",
    "    x_example, y_example = X_train[17], y_train[17]\n",
    "    print(\"x:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in x_example]), x_example))\n",
    "    print(\"\\ny:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in y_example]), y_example))\n",
    "\n",
    "    return X_train, y_train\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    X_train, y_train = getSentenceData('data/reddit.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2924187f8edd4dc2af9dda14dceb3e192bc5d2697c248af5fd7615f78118c875"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
