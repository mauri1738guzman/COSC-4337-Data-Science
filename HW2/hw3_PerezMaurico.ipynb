{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "y_hat = tf.constant(36)     #-- Define y_hat constant set to 36\n",
    "y= tf.constant(39)          #-- Define y. Set to 39\n",
    "\n",
    "loss = (y-y_hat)**2         #-- Calculate the loss\n",
    "tf.print(loss)              #-- print the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4]]\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "[[1 1 1]\n",
      " [1 1 1]\n",
      " [1 1 1]]\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "[[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(4, shape=(1,1), dtype =tf.float32)\n",
    "tf.print(x)\n",
    "\n",
    "x = tf.constant([[1,2,3], [4,5,6]])\n",
    "tf.print(x)\n",
    "\n",
    "x = tf.ones((3,3))\n",
    "tf.print(x)\n",
    "\n",
    "x = tf.zeros((3,3))\n",
    "tf.print(x)\n",
    "\n",
    "x = tf.eye(3)\n",
    "tf.print(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 10 10]\n",
      "[-8 -6 -4]\n",
      "[0.1111111111111111 0.25 0.42857142857142855]\n",
      "[9 16 21]\n",
      "46\n",
      "[1 32 243]\n",
      "[[-0.541303515 1.9507544 -1.66218197 4.53199387]\n",
      " [1.78214574 0.718164206 0.93665427 -2.16695142]]\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([1,2,3])\n",
    "y = tf.constant([9,8,7])\n",
    "\n",
    "z = x + y\n",
    "tf.print(z)\n",
    "\n",
    "z = x - y\n",
    "tf.print(z)\n",
    "\n",
    "z = x / y\n",
    "tf.print(z)\n",
    "\n",
    "z = x * y\n",
    "tf.print(z)\n",
    "\n",
    "z = tf.tensordot(x,y,axes=1)\n",
    "tf.print(z)\n",
    "\n",
    "z = x ** 5\n",
    "tf.print(z)\n",
    "\n",
    "x = tf.random.normal((2,3))\n",
    "y = tf.random.normal((3,4))\n",
    "\n",
    "z = tf.matmul(x,y)\n",
    "tf.print(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 1 2 3 1 2 3], shape=(8,), dtype=int32)\n",
      "tf.Tensor([1 1 2 3 1 2 3], shape=(7,), dtype=int32)\n",
      "tf.Tensor([1 1], shape=(2,), dtype=int32)\n",
      "tf.Tensor([0 1 3 2], shape=(4,), dtype=int32)\n",
      "tf.Tensor([3 2 1 3 2 1 1 0], shape=(8,), dtype=int32)\n",
      "tf.Tensor([0 2], shape=(2,), dtype=int32)\n",
      "tf.Tensor([1 2], shape=(2,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[1 2]\n",
      " [3 4]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([0,1,1,2,3,1,2,3])\n",
    "print(x[:])\n",
    "print(x[1:])\n",
    "print(x[1:3])\n",
    "print(x[::2])\n",
    "print(x[::-1])\n",
    "\n",
    "indices = tf.constant([0,3])\n",
    "x_ind = tf.gather(x, indices)\n",
    "print(x_ind)\n",
    "\n",
    "x = tf.constant([[1,2], [3,4], [5,6]])\n",
    "print(x[0,:])\n",
    "print(x[0:2, :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6 7 8], shape=(9,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[0 1 2]\n",
      " [3 4 5]\n",
      " [6 7 8]], shape=(3, 3), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[0 3 6]\n",
      " [1 4 7]\n",
      " [2 5 8]], shape=(3, 3), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.range(9)\n",
    "print(x)\n",
    "\n",
    "x= tf.reshape(x,(3,3))\n",
    "print(x)\n",
    "\n",
    "x = tf.transpose(x, perm=[1,0])\n",
    "print(x)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_function():\n",
    "    W = tf.random.uniform(\n",
    "        (4,3),\n",
    "        minval=0,\n",
    "        maxval=100,\n",
    "        dtype=tf.dtypes.int32,\n",
    "        seed=None,\n",
    "        name=None\n",
    "    )\n",
    "    # print(\"This is variable W: \")\n",
    "    # print(W)\n",
    "\n",
    "    X = tf.random.uniform(\n",
    "        (3,1),\n",
    "        minval=0,\n",
    "        maxval=100,\n",
    "        dtype=tf.dtypes.int32,\n",
    "        seed=None,\n",
    "        name=None\n",
    "    )\n",
    "    # print(\"This is variable X: \")\n",
    "    # print(X)\n",
    "\n",
    "    b = tf.random.uniform(\n",
    "        (4,1),\n",
    "        minval=0,\n",
    "        maxval=100,\n",
    "        dtype=tf.dtypes.int32,\n",
    "        seed=None,\n",
    "        name=None\n",
    "    )\n",
    "\n",
    "    # print(\"This is variable b: \")\n",
    "    # print(b)\n",
    "\n",
    "    mulWX = tf.matmul(W,X) \n",
    "    final = tf.add(mulWX, b)\n",
    "    return final\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 6s 0us/step\n",
      "(60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(-1, 28 *28).astype(\"float32\") / 255.0\n",
    "x_test = x_test.reshape(-1, 28 * 28).astype(\"float32\") / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Dense(2, activation='relu'),\n",
    "        layers.Dense(3, activation='relu'),\n",
    "        layers.Dense(4),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.core.dense.Dense at 0x27912d196a0>,\n",
       " <keras.layers.core.dense.Dense at 0x27912d194f0>,\n",
       " <keras.layers.core.dense.Dense at 0x27912d19670>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mauri\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 - 5s - loss: 1.4706 - accuracy: 0.6730 - 5s/epoch - 3ms/step\n",
      "313/313 - 1s - loss: 0.7904 - accuracy: 0.8448 - 571ms/epoch - 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.790351390838623, 0.8447999954223633]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating our layers\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Dense(200, activation='relu'),\n",
    "        layers.Dense(200, activation='relu'),\n",
    "        layers.Dense(10),\n",
    "    ]\n",
    ")\n",
    "\n",
    "#-- telling keras how to configure the training part of our network. For example\n",
    "#-- what loss do you want to use for training, etc\n",
    "\n",
    "model.compile(\n",
    "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer = keras.optimizers.Adam(lr = 0.00001),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "#-- fit train our model similar to sklearn's fit\n",
    "model.fit(x_train, y_train, batch_size=32, epochs = 1, verbose=2)\n",
    "#-- evaluate using the test data\n",
    "model.evaluate(x_test, y_test, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- The reason the fully connected layer was of size 10 was in order to print out a shape of [None, 10]\n",
    "#-- cross entropy is great for multi-classification models and in this case we have 0-9\n",
    "#-- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 - 4s - loss: 1.5103 - accuracy: 0.6433 - 4s/epoch - 2ms/step\n",
      "Epoch 2/10\n",
      "1875/1875 - 4s - loss: 0.6095 - accuracy: 0.8589 - 4s/epoch - 2ms/step\n",
      "Epoch 3/10\n",
      "1875/1875 - 4s - loss: 0.4303 - accuracy: 0.8888 - 4s/epoch - 2ms/step\n",
      "Epoch 4/10\n",
      "1875/1875 - 4s - loss: 0.3637 - accuracy: 0.9021 - 4s/epoch - 2ms/step\n",
      "Epoch 5/10\n",
      "1875/1875 - 4s - loss: 0.3265 - accuracy: 0.9103 - 4s/epoch - 2ms/step\n",
      "Epoch 6/10\n",
      "1875/1875 - 4s - loss: 0.3007 - accuracy: 0.9168 - 4s/epoch - 2ms/step\n",
      "Epoch 7/10\n",
      "1875/1875 - 4s - loss: 0.2810 - accuracy: 0.9218 - 4s/epoch - 2ms/step\n",
      "Epoch 8/10\n",
      "1875/1875 - 4s - loss: 0.2649 - accuracy: 0.9264 - 4s/epoch - 2ms/step\n",
      "Epoch 9/10\n",
      "1875/1875 - 4s - loss: 0.2512 - accuracy: 0.9304 - 4s/epoch - 2ms/step\n",
      "Epoch 10/10\n",
      "1875/1875 - 4s - loss: 0.2391 - accuracy: 0.9337 - 4s/epoch - 2ms/step\n",
      "313/313 - 1s - loss: 0.2302 - accuracy: 0.9348 - 521ms/epoch - 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.23022285103797913, 0.9348000288009644]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating our layers\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Dense(200, activation='relu'),\n",
    "        layers.Dense(200, activation='relu'),\n",
    "        layers.Dense(10),\n",
    "    ]\n",
    ")\n",
    "\n",
    "#-- telling keras how to configure the training part of our network. For example\n",
    "#-- what loss do you want to use for training, etc\n",
    "\n",
    "model.compile(\n",
    "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer = keras.optimizers.Adam(lr = 0.00001),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "#-- fit train our model similar to sklearn's fit\n",
    "model.fit(x_train, y_train, batch_size=32, epochs = 10, verbose=2)\n",
    "#-- evaluate using the test data\n",
    "model.evaluate(x_test, y_test, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- As we can tell here as we raise the epochs we realize that each iteration the accuracy goes up \n",
    "#-- In this scenario I ran it till 10 epochs but we can clearly see if we ran 50 or 100 we would have over 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mauri\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 - 7s - loss: 0.1857 - accuracy: 0.9438 - 7s/epoch - 4ms/step\n",
      "Epoch 2/5\n",
      "1875/1875 - 7s - loss: 0.0789 - accuracy: 0.9750 - 7s/epoch - 4ms/step\n",
      "Epoch 3/5\n",
      "1875/1875 - 7s - loss: 0.0554 - accuracy: 0.9821 - 7s/epoch - 4ms/step\n",
      "Epoch 4/5\n",
      "1875/1875 - 7s - loss: 0.0408 - accuracy: 0.9866 - 7s/epoch - 4ms/step\n",
      "Epoch 5/5\n",
      "1875/1875 - 7s - loss: 0.0321 - accuracy: 0.9897 - 7s/epoch - 4ms/step\n",
      "313/313 - 1s - loss: 0.0831 - accuracy: 0.9795 - 686ms/epoch - 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08305422216653824, 0.9794999957084656]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(28 * 28))\n",
    "x = layers.Dense(512, activation=\"relu\")(inputs)            #-- pass in inputs\n",
    "x = layers.Dense(256, activation=\"relu\")(x)                 #-- pass in previous layer's output\n",
    "outputs = layers.Dense(10, activation=\"softmax\")(x)         #-- pass in previous layer's output\n",
    "model = keras.Model(inputs = inputs, outputs = outputs)\n",
    "\n",
    "model.compile(\n",
    "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    optimizer = keras.optimizers.Adam(lr = 0.001),\n",
    "    metrics=['accuracy'],\n",
    "\n",
    ")\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=5, verbose=2)\n",
    "model.evaluate(x_test, y_test, batch_size=32, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- There is less loss in functional and greaterr accuracy\n",
    "#-- The accuracy was 97.9% and you could prob raise this score by raising the epochs count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------BELOW IS THE TUTORIAL ON HOW TO WORK WITH CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Make numpy values easier to read.\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Length</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>Whole weight</th>\n",
       "      <th>Shucked weight</th>\n",
       "      <th>Viscera weight</th>\n",
       "      <th>Shell weight</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.435</td>\n",
       "      <td>0.335</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.334</td>\n",
       "      <td>0.1355</td>\n",
       "      <td>0.0775</td>\n",
       "      <td>0.0965</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.585</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.3545</td>\n",
       "      <td>0.2075</td>\n",
       "      <td>0.2250</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.655</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.160</td>\n",
       "      <td>1.092</td>\n",
       "      <td>0.3960</td>\n",
       "      <td>0.2825</td>\n",
       "      <td>0.3700</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.545</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.768</td>\n",
       "      <td>0.2940</td>\n",
       "      <td>0.1495</td>\n",
       "      <td>0.2600</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.545</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.879</td>\n",
       "      <td>0.3740</td>\n",
       "      <td>0.1695</td>\n",
       "      <td>0.2300</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Length  Diameter  Height  Whole weight  Shucked weight  Viscera weight  \\\n",
       "0   0.435     0.335   0.110         0.334          0.1355          0.0775   \n",
       "1   0.585     0.450   0.125         0.874          0.3545          0.2075   \n",
       "2   0.655     0.510   0.160         1.092          0.3960          0.2825   \n",
       "3   0.545     0.425   0.125         0.768          0.2940          0.1495   \n",
       "4   0.545     0.420   0.130         0.879          0.3740          0.1695   \n",
       "\n",
       "   Shell weight  Age  \n",
       "0        0.0965    7  \n",
       "1        0.2250    6  \n",
       "2        0.3700   14  \n",
       "3        0.2600   16  \n",
       "4        0.2300   13  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#-- Here we will read the file from a online storage and assing the column names\n",
    "abalone_train = pd.read_csv(\n",
    "    \"https://storage.googleapis.com/download.tensorflow.org/data/abalone_train.csv\",\n",
    "    names=[\"Length\", \"Diameter\", \"Height\", \"Whole weight\", \"Shucked weight\",\n",
    "           \"Viscera weight\", \"Shell weight\", \"Age\"])\n",
    "\n",
    "abalone_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Main point here is to decide the age \n",
    "abalone_features = abalone_train.copy()\n",
    "abalone_labels = abalone_features.pop('Age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.435, 0.335, 0.11 , ..., 0.136, 0.077, 0.097],\n",
       "       [0.585, 0.45 , 0.125, ..., 0.354, 0.207, 0.225],\n",
       "       [0.655, 0.51 , 0.16 , ..., 0.396, 0.282, 0.37 ],\n",
       "       ...,\n",
       "       [0.53 , 0.42 , 0.13 , ..., 0.374, 0.167, 0.249],\n",
       "       [0.395, 0.315, 0.105, ..., 0.118, 0.091, 0.119],\n",
       "       [0.45 , 0.355, 0.12 , ..., 0.115, 0.067, 0.16 ]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-- place in numpy array\n",
    "abalone_features = np.array(abalone_features)\n",
    "abalone_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Create a regression model, sequential is great for one input\n",
    "abalone_model = tf.keras.Sequential([\n",
    "  layers.Dense(64),\n",
    "  layers.Dense(1)\n",
    "])\n",
    "\n",
    "abalone_model.compile(loss = tf.keras.losses.MeanSquaredError(),\n",
    "                      optimizer = tf.keras.optimizers.Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "Epoch 1/10\n",
      "104/104 [==============================] - 2s 10ms/step - loss: 64.2275\n",
      "Epoch 2/10\n",
      "104/104 [==============================] - 1s 11ms/step - loss: 11.8205\n",
      "Epoch 3/10\n",
      "104/104 [==============================] - 1s 11ms/step - loss: 8.4791\n",
      "Epoch 4/10\n",
      "104/104 [==============================] - 1s 11ms/step - loss: 7.9715\n",
      "Epoch 5/10\n",
      "104/104 [==============================] - 1s 11ms/step - loss: 7.5173\n",
      "Epoch 6/10\n",
      "104/104 [==============================] - 1s 11ms/step - loss: 7.1678\n",
      "Epoch 7/10\n",
      "104/104 [==============================] - 1s 11ms/step - loss: 6.8925\n",
      "Epoch 8/10\n",
      "104/104 [==============================] - 1s 10ms/step - loss: 6.7000\n",
      "Epoch 9/10\n",
      "104/104 [==============================] - 1s 10ms/step - loss: 6.5638\n",
      "Epoch 10/10\n",
      "104/104 [==============================] - 1s 10ms/step - loss: 6.4562\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27921779190>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-- Here we train the model with 10 epochs \n",
    "abalone_model.fit(abalone_features, abalone_labels, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- By normalizing the data we initially calculate the mean and variance of each column\n",
    "normalize = layers.Normalization()\n",
    "normalize.adapt(abalone_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "Epoch 1/10\n",
      "104/104 [==============================] - 1s 11ms/step - loss: 92.5313\n",
      "Epoch 2/10\n",
      "104/104 [==============================] - 1s 11ms/step - loss: 53.3551\n",
      "Epoch 3/10\n",
      "104/104 [==============================] - 1s 11ms/step - loss: 16.2490\n",
      "Epoch 4/10\n",
      "104/104 [==============================] - 1s 11ms/step - loss: 5.8163\n",
      "Epoch 5/10\n",
      "104/104 [==============================] - 1s 11ms/step - loss: 5.0189\n",
      "Epoch 6/10\n",
      "104/104 [==============================] - 1s 11ms/step - loss: 4.9889\n",
      "Epoch 7/10\n",
      "104/104 [==============================] - 1s 11ms/step - loss: 4.9502\n",
      "Epoch 8/10\n",
      "104/104 [==============================] - 1s 10ms/step - loss: 4.9540\n",
      "Epoch 9/10\n",
      "104/104 [==============================] - 1s 11ms/step - loss: 4.9166\n",
      "Epoch 10/10\n",
      "104/104 [==============================] - 1s 10ms/step - loss: 4.9295\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27921aa2a90>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-- Normalizing above has created another layer so we fit the data\n",
    "norm_abalone_model = tf.keras.Sequential([\n",
    "  normalize,\n",
    "  layers.Dense(64),\n",
    "  layers.Dense(1)\n",
    "])\n",
    "\n",
    "norm_abalone_model.compile(loss = tf.keras.losses.MeanSquaredError(),\n",
    "                           optimizer = tf.keras.optimizers.Adam())\n",
    "\n",
    "norm_abalone_model.fit(abalone_features, abalone_labels, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>n_siblings_spouses</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>class</th>\n",
       "      <th>deck</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>Third</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>First</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>Third</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>First</td>\n",
       "      <td>C</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>Third</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Queenstown</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived     sex   age  n_siblings_spouses  parch     fare  class     deck  \\\n",
       "0         0    male  22.0                   1      0   7.2500  Third  unknown   \n",
       "1         1  female  38.0                   1      0  71.2833  First        C   \n",
       "2         1  female  26.0                   0      0   7.9250  Third  unknown   \n",
       "3         1  female  35.0                   1      0  53.1000  First        C   \n",
       "4         0    male  28.0                   0      0   8.4583  Third  unknown   \n",
       "\n",
       "   embark_town alone  \n",
       "0  Southampton     n  \n",
       "1    Cherbourg     n  \n",
       "2  Southampton     y  \n",
       "3  Southampton     n  \n",
       "4   Queenstown     y  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic = pd.read_csv(\"https://storage.googleapis.com/tf-datasets/titanic/train.csv\")\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Here we will pop the survived because that is what we want to know\n",
    "titanic_features = titanic.copy()\n",
    "titanic_labels = titanic_features.pop('survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None,) dtype=float32 (created by layer 'tf.__operators__.add')>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-- This will create a pre defined tensor to use later w/ float variables \n",
    "# Create a symbolic input\n",
    "input = tf.keras.Input(shape=(), dtype=tf.float32)\n",
    "\n",
    "# Perform a calculation using the input\n",
    "result = 2*input + 1\n",
    "\n",
    "# the result doesn't have a value\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc = tf.keras.Model(inputs=input, outputs=result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n",
      "5.0\n"
     ]
    }
   ],
   "source": [
    "#-- 1 is put into the equation\n",
    "#-- 2 is pit into the next function\n",
    "print(calc(1).numpy())\n",
    "print(calc(2).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sex': <KerasTensor: shape=(None, 1) dtype=string (created by layer 'sex')>,\n",
       " 'age': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'age')>,\n",
       " 'n_siblings_spouses': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'n_siblings_spouses')>,\n",
       " 'parch': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'parch')>,\n",
       " 'fare': <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'fare')>,\n",
       " 'class': <KerasTensor: shape=(None, 1) dtype=string (created by layer 'class')>,\n",
       " 'deck': <KerasTensor: shape=(None, 1) dtype=string (created by layer 'deck')>,\n",
       " 'embark_town': <KerasTensor: shape=(None, 1) dtype=string (created by layer 'embark_town')>,\n",
       " 'alone': <KerasTensor: shape=(None, 1) dtype=string (created by layer 'alone')>}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-- Built preprocessing model w/ type of variables\n",
    "\n",
    "inputs = {}\n",
    "\n",
    "for name, column in titanic_features.items():\n",
    "  dtype = column.dtype\n",
    "  if dtype == object:\n",
    "    dtype = tf.string\n",
    "  else:\n",
    "    dtype = tf.float32\n",
    "\n",
    "  inputs[name] = tf.keras.Input(shape=(1,), name=name, dtype=dtype)\n",
    "\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 4) dtype=float32 (created by layer 'normalization_1')>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-- Here concatentaion takes places \n",
    "numeric_inputs = {name:input for name,input in inputs.items()\n",
    "                  if input.dtype==tf.float32}\n",
    "\n",
    "x = layers.Concatenate()(list(numeric_inputs.values()))\n",
    "norm = layers.Normalization()\n",
    "norm.adapt(np.array(titanic[numeric_inputs.keys()]))\n",
    "all_numeric_inputs = norm(x)\n",
    "\n",
    "all_numeric_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- This we will concatenate later\n",
    "preprocessed_inputs = [all_numeric_inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Here we will create one hot vector for each input\n",
    "for name, input in inputs.items():\n",
    "  if input.dtype == tf.float32:\n",
    "    continue\n",
    "\n",
    "  lookup = layers.StringLookup(vocabulary=np.unique(titanic_features[name]))\n",
    "  one_hot = layers.CategoryEncoding(num_tokens=lookup.vocabulary_size())\n",
    "\n",
    "  x = lookup(input)\n",
    "  x = one_hot(x)\n",
    "  preprocessed_inputs.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "#-- This will build a model but requires further libraries to print\n",
    "preprocessed_inputs_cat = layers.Concatenate()(preprocessed_inputs)\n",
    "\n",
    "titanic_preprocessing = tf.keras.Model(inputs, preprocessed_inputs_cat)\n",
    "\n",
    "tf.keras.utils.plot_model(model = titanic_preprocessing , rankdir=\"LR\", dpi=72, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Here is a dictionary of tensors in order to specify bc library doesn't know what to do with it \n",
    "titanic_features_dict = {name: np.array(value) \n",
    "                         for name, value in titanic_features.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 28), dtype=float32, numpy=\n",
       "array([[-0.61 ,  0.395, -0.479, -0.497,  0.   ,  0.   ,  1.   ,  0.   ,\n",
       "         0.   ,  0.   ,  1.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,\n",
       "         0.   ,  0.   ,  0.   ,  1.   ,  0.   ,  0.   ,  0.   ,  1.   ,\n",
       "         0.   ,  0.   ,  1.   ,  0.   ]], dtype=float32)>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-- This brings out the numeric and string one hots\n",
    "features_dict = {name:values[:1] for name, values in titanic_features_dict.items()}\n",
    "titanic_preprocessing(features_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Time to build model\n",
    "def titanic_model(preprocessing_head, inputs):\n",
    "  body = tf.keras.Sequential([\n",
    "    layers.Dense(64),\n",
    "    layers.Dense(1)\n",
    "  ])\n",
    "\n",
    "  preprocessed_inputs = preprocessing_head(inputs)\n",
    "  result = body(preprocessed_inputs)\n",
    "  model = tf.keras.Model(inputs, result)\n",
    "\n",
    "  model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                optimizer=tf.keras.optimizers.Adam())\n",
    "  return model\n",
    "\n",
    "titanic_model = titanic_model(titanic_preprocessing, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n",
      "Epoch 1/10\n",
      "20/20 [==============================] - 3s 56ms/step - loss: 0.6334\n",
      "Epoch 2/10\n",
      "20/20 [==============================] - 2s 96ms/step - loss: 0.5376\n",
      "Epoch 3/10\n",
      "20/20 [==============================] - 1s 55ms/step - loss: 0.4872\n",
      "Epoch 4/10\n",
      "20/20 [==============================] - 2s 72ms/step - loss: 0.4593\n",
      "Epoch 5/10\n",
      "20/20 [==============================] - 2s 79ms/step - loss: 0.4438\n",
      "Epoch 6/10\n",
      "20/20 [==============================] - 2s 66ms/step - loss: 0.4337\n",
      "Epoch 7/10\n",
      "20/20 [==============================] - 2s 84ms/step - loss: 0.4278\n",
      "Epoch 8/10\n",
      "20/20 [==============================] - 2s 80ms/step - loss: 0.4253\n",
      "Epoch 9/10\n",
      "20/20 [==============================] - 2s 79ms/step - loss: 0.4247\n",
      "Epoch 10/10\n",
      "20/20 [==============================] - 1s 59ms/step - loss: 0.4222\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27922b43a30>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-- Pass in dictionaries after training the model\n",
    "titanic_model.fit(x=titanic_features_dict, y=titanic_labels, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: test\\assets\n"
     ]
    }
   ],
   "source": [
    "#-- reload it to run the preprocessing because it belongs to the model\n",
    "titanic_model.save('test')\n",
    "reloaded = tf.keras.models.load_model('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[-1.89]], shape=(1, 1), dtype=float32)\n",
      "tf.Tensor([[-1.89]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "features_dict = {name:values[:1] for name, values in titanic_features_dict.items()}\n",
    "\n",
    "before = titanic_model(features_dict)\n",
    "after = reloaded(features_dict)\n",
    "assert (before-after)<1e-3\n",
    "print(before)\n",
    "print(after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--  create slices with index saved of each feature \n",
    "import itertools\n",
    "\n",
    "def slices(features):\n",
    "  for i in itertools.count():\n",
    "    # For each feature take index `i`\n",
    "    example = {name:values[i] for name, values in features.items()}\n",
    "    yield example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex                : male\n",
      "age                : 22.0\n",
      "n_siblings_spouses : 1\n",
      "parch              : 0\n",
      "fare               : 7.25\n",
      "class              : Third\n",
      "deck               : unknown\n",
      "embark_town        : Southampton\n",
      "alone              : n\n"
     ]
    }
   ],
   "source": [
    "#-- This will print the first example\n",
    "for example in slices(titanic_features_dict):\n",
    "  for name, value in example.items():\n",
    "    print(f\"{name:19s}: {value}\")\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- This line does what was done in the last two boxes above\n",
    "features_ds = tf.data.Dataset.from_tensor_slices(titanic_features_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex                : b'male'\n",
      "age                : 22.0\n",
      "n_siblings_spouses : 1\n",
      "parch              : 0\n",
      "fare               : 7.25\n",
      "class              : b'Third'\n",
      "deck               : b'unknown'\n",
      "embark_town        : b'Southampton'\n",
      "alone              : b'n'\n"
     ]
    }
   ],
   "source": [
    "for example in features_ds:\n",
    "  for name, value in example.items():\n",
    "    print(f\"{name:19s}: {value}\")\n",
    "  break\n",
    "#-- This will iterate just the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- create a dataset pairs\n",
    "titanic_ds = tf.data.Dataset.from_tensor_slices((titanic_features_dict, titanic_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_batches = titanic_ds.shuffle(len(titanic_labels)).batch(32)\n",
    "#-- Shuffle data around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "20/20 [==============================] - 1s 2ms/step - loss: 0.4237\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4209\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4213\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4210\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4203\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x279249b4370>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-- Pass whole dataset\n",
    "titanic_model.fit(titanic_batches, epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tf-datasets/titanic/train.csv\n",
      "30874/30874 [==============================] - 0s 1us/step\n"
     ]
    }
   ],
   "source": [
    "titanic_file_path = tf.keras.utils.get_file(\"train.csv\", \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- This is creating a tf dataset\n",
    "titanic_csv_ds = tf.data.experimental.make_csv_dataset(\n",
    "    titanic_file_path,\n",
    "    batch_size=5, # Artificially small to make examples easier to show.\n",
    "    label_name='survived',\n",
    "    num_epochs=1,\n",
    "    ignore_errors=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex                 : [b'female' b'male' b'male' b'male' b'male']\n",
      "age                 : [15. 21. 21. 26. 28.]\n",
      "n_siblings_spouses  : [1 0 0 0 0]\n",
      "parch               : [0 0 0 0 0]\n",
      "fare                : [14.454  7.796  7.775  7.775  8.05 ]\n",
      "class               : [b'Third' b'Third' b'Third' b'Third' b'Third']\n",
      "deck                : [b'unknown' b'unknown' b'unknown' b'unknown' b'unknown']\n",
      "embark_town         : [b'Cherbourg' b'Southampton' b'Southampton' b'Southampton' b'Southampton']\n",
      "alone               : [b'n' b'y' b'y' b'y' b'y']\n",
      "\n",
      "label               : [1 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "for batch, label in titanic_csv_ds.take(1):\n",
    "  for key, value in batch.items():\n",
    "    print(f\"{key:20s}: {value}\")\n",
    "  print()\n",
    "  print(f\"{'label':20s}: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex                 : [b'male' b'male' b'male' b'female' b'male']\n",
      "age                 : [24. 62. 35. 28. 31.]\n",
      "n_siblings_spouses  : [0 0 0 0 1]\n",
      "parch               : [0 0 0 2 0]\n",
      "fare                : [  7.896  10.5   512.329  22.358  57.   ]\n",
      "class               : [b'Third' b'Second' b'First' b'Third' b'First']\n",
      "deck                : [b'unknown' b'unknown' b'B' b'unknown' b'B']\n",
      "embark_town         : [b'Southampton' b'Southampton' b'Cherbourg' b'Cherbourg' b'Southampton']\n",
      "alone               : [b'y' b'y' b'y' b'n' b'n']\n",
      "\n",
      "label               : [0 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "#-- running the code above twice will return different results \n",
    "for batch, label in titanic_csv_ds.take(1):\n",
    "  for key, value in batch.items():\n",
    "    print(f\"{key:20s}: {value}\")\n",
    "  print()\n",
    "  print(f\"{'label':20s}: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://archive.ics.uci.edu/ml/machine-learning-databases/00492/Metro_Interstate_Traffic_Volume.csv.gz\n",
      "405373/405373 [==============================] - 0s 1us/step\n"
     ]
    }
   ],
   "source": [
    "traffic_volume_csv_gz = tf.keras.utils.get_file(\n",
    "    'Metro_Interstate_Traffic_Volume.csv.gz', \n",
    "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/00492/Metro_Interstate_Traffic_Volume.csv.gz\",\n",
    "    cache_dir='.', cache_subdir='traffic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "holiday             : [b'None' b'None' b'None' b'None' b'None']\n",
      "temp                : [275.15 274.38 282.23 290.57 265.04]\n",
      "rain_1h             : [0. 0. 0. 0. 0.]\n",
      "snow_1h             : [0. 0. 0. 0. 0.]\n",
      "clouds_all          : [ 1 90 90 90 75]\n",
      "weather_main        : [b'Clear' b'Haze' b'Clouds' b'Clouds' b'Mist']\n",
      "weather_description : [b'sky is clear' b'haze' b'overcast clouds' b'overcast clouds' b'mist']\n",
      "date_time           : [b'2013-04-16 22:00:00' b'2012-12-01 18:00:00' b'2012-10-14 05:00:00'\n",
      " b'2013-06-30 09:00:00' b'2013-01-06 22:00:00']\n",
      "\n",
      "label               : [2142 4788  505 3245 1384]\n"
     ]
    }
   ],
   "source": [
    "#-- Set compression type in order to better read file\n",
    "traffic_volume_csv_gz_ds = tf.data.experimental.make_csv_dataset(\n",
    "    traffic_volume_csv_gz,\n",
    "    batch_size=256,\n",
    "    label_name='traffic_volume',\n",
    "    num_epochs=1,\n",
    "    compression_type=\"GZIP\")\n",
    "\n",
    "for batch, label in traffic_volume_csv_gz_ds.take(1):\n",
    "  for key, value in batch.items():\n",
    "    print(f\"{key:20s}: {value[:5]}\")\n",
    "  print()\n",
    "  print(f\"{'label':20s}: {label[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n",
      "...............................................................................................\n"
     ]
    }
   ],
   "source": [
    "#-- Caching-- This can be the bottle neck of small dataset\n",
    "%time\n",
    "for i, (batch, label) in enumerate(traffic_volume_csv_gz_ds.repeat(20)):\n",
    "  if i % 40 == 0:\n",
    "    print('.', end='')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n",
      "...............................................................................................\n"
     ]
    }
   ],
   "source": [
    "#-- Here we will shuffle data \n",
    "%time\n",
    "caching = traffic_volume_csv_gz_ds.cache().shuffle(1000)\n",
    "\n",
    "for i, (batch, label) in enumerate(caching.shuffle(1000).repeat(20)):\n",
    "  if i % 40 == 0:\n",
    "    print('.', end='')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n",
      "WARNING:tensorflow:From C:\\Users\\mauri\\AppData\\Local\\Temp\\ipykernel_5836\\1889687430.py:3: snapshot (from tensorflow.python.data.experimental.ops.snapshot) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.snapshot(...)`.\n",
      "...............................................................................................\n"
     ]
    }
   ],
   "source": [
    "#-- snapshot is only for temp storage\n",
    "%time\n",
    "snapshot = tf.data.experimental.snapshot('titanic.tfsnap')\n",
    "snapshotting = traffic_volume_csv_gz_ds.apply(snapshot).shuffle(1000)\n",
    "\n",
    "for i, (batch, label) in enumerate(snapshotting.shuffle(1000).repeat(20)):\n",
    "  if i % 40 == 0:\n",
    "    print('.', end='')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://archive.ics.uci.edu/ml/machine-learning-databases/00417/fonts.zip\n",
      "160313983/160313983 [==============================] - 75s 0us/step\n"
     ]
    }
   ],
   "source": [
    "fonts_zip = tf.keras.utils.get_file(\n",
    "    'fonts.zip',  \"https://archive.ics.uci.edu/ml/machine-learning-databases/00417/fonts.zip\",\n",
    "    cache_dir='.', cache_subdir='fonts',\n",
    "    extract=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fonts\\\\AGENCY.csv',\n",
       " 'fonts\\\\ARIAL.csv',\n",
       " 'fonts\\\\BAITI.csv',\n",
       " 'fonts\\\\BANKGOTHIC.csv',\n",
       " 'fonts\\\\BASKERVILLE.csv',\n",
       " 'fonts\\\\BAUHAUS.csv',\n",
       " 'fonts\\\\BELL.csv',\n",
       " 'fonts\\\\BERLIN.csv',\n",
       " 'fonts\\\\BERNARD.csv',\n",
       " 'fonts\\\\BITSTREAMVERA.csv']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-- downloading many files\n",
    "import pathlib\n",
    "font_csvs =  sorted(str(p) for p in pathlib.Path('fonts').glob(\"*.csv\"))\n",
    "\n",
    "font_csvs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(font_csvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- function sees how many files are read in parallel and how many are interleaved\n",
    "fonts_ds = tf.data.experimental.make_csv_dataset(\n",
    "    file_pattern = \"fonts/*.csv\",\n",
    "    batch_size=10, num_epochs=1,\n",
    "    num_parallel_reads=20,\n",
    "    shuffle_buffer_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "font                : [b'BRADLEY' b'PERPETUA' b'BODONI' b'BAITI' b'PERPETUA' b'PRISTINA'\n",
      " b'TECHNIC' b'E13B' b'MONOSPAC821' b'TW']\n",
      "fontVariant         : [b'BRADLEY HAND ITC' b'PERPETUA TITLING MT' b'BODONI MT POSTER COMPRESSED'\n",
      " b'MONGOLIAN BAITI' b'PERPETUA TITLING MT' b'PRISTINA' b'TECHNICLITE'\n",
      " b'scanned' b'MONOSPAC821 BT' b'TW CEN MT CONDENSED EXTRA BOLD']\n",
      "m_label             : [   44   222    57  8250   251   235 61639    48   321   711]\n",
      "strength            : [0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4 0.4]\n",
      "italic              : [0 1 0 1 1 0 0 0 0 0]\n",
      "orientation         : [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "m_top               : [71 36 33 46 21 37 34  0 39 36]\n",
      "m_left              : [20 23 21 24 31 21 24  0 19 20]\n",
      "originalH           : [14 48 49 32 64 42 57 16 46 10]\n",
      "originalW           : [11 40 16 20 56 19 32  8 37 22]\n",
      "h                   : [20 20 20 20 20 20 20 20 20 20]\n",
      "w                   : [20 20 20 20 20 20 20 20 20 20]\n",
      "r0c0                : [  1   1   1   1   1   1   1   0   1 255]\n",
      "r0c1                : [  1   1   1   1   1   1   1   0   1 255]\n",
      "r0c2                : [  1   1   1   1   1   1   1   0   1 255]\n",
      "r0c3                : [  1   1  12   1   1   1   1   0  56 255]\n",
      "...\n",
      "[total: 412 features]\n"
     ]
    }
   ],
   "source": [
    "#-- Here images are flattened to a single row\n",
    "for features in fonts_ds.take(1):\n",
    "  for i, (name, value) in enumerate(features.items()):\n",
    "    if i>15:\n",
    "      break\n",
    "    print(f\"{name:20s}: {value}\")\n",
    "print('...')\n",
    "print(f\"[total: {len(features)} features]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- This parses the images into its own column \n",
    "import re\n",
    "\n",
    "def make_images(features):\n",
    "  image = [None]*400\n",
    "  new_feats = {}\n",
    "\n",
    "  for name, value in features.items():\n",
    "    match = re.match('r(\\d+)c(\\d+)', name)\n",
    "    if match:\n",
    "      image[int(match.group(1))*20+int(match.group(2))] = value\n",
    "    else:\n",
    "      new_feats[name] = value\n",
    "\n",
    "  image = tf.stack(image, axis=0)\n",
    "  image = tf.reshape(image, [20, 20, -1])\n",
    "  new_feats['image'] = image\n",
    "\n",
    "  return new_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Then you have to apply the function above to each batch\n",
    "fonts_image_ds = fonts_ds.map(make_images)\n",
    "\n",
    "for features in fonts_image_ds.take(1):\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mauri\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\IPython\\core\\events.py:89: UserWarning: Glyph 43856 (\\N{LATIN SMALL LETTER UI}) missing from current font.\n",
      "  func(*args, **kwargs)\n",
      "C:\\Users\\mauri\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\IPython\\core\\pylabtools.py:151: UserWarning: Glyph 43856 (\\N{LATIN SMALL LETTER UI}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkEAAAJQCAYAAACJjrCTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAABJ0AAASdAHeZh94AAAzx0lEQVR4nO3deZyd893/8c/3nNlnsu+rRBZiDSFEaRBuu0YtdVcXirSoteimRbVutLaW2Kq60FbDr0VbgpTSJhFUlBBLhJBVZLJMZj9z/f6Ycd+pxvvK6TVnyXxez8ejj6m8z7muT+bMOfPONTOfCVEUGQAAgDepQg8AAABQCJQgAADgEiUIAAC4RAkCAAAuUYIAAIBLlCAAAOASJQgAALhECQIAAC5RgvIghLBDCGFNCOHeQs8CAADaUYLyIIqiV8zsKDM7OoTw1ULPAwAAKEF5E0XR383sSDPrFkIoLfQ8AAB4RwnKoyiKZkVR9D9RFLUUehYAQPEIIZwfQohCCI+FECpCCN1CCMeGEH5Y6Nm6MkpQnoQQvt3xAR6FELYr9DxAoYUQpoQQfh9CWBFCaAohLAshzAwhHF7o2YB8i6LoejO73swOMrNXzGyBmR1hZnNCCHyuzhHesXkQQghmdpqZRR1/dHoBxwEKLoRwjZk9bmZ7mNmDZnatmf3JzPqZ2f6FmwzIvxDCt0IIL1t7Afp/ZjbSzGab2alRFP2/KIraCjpgFxaiKIq/FRIJIRxiZo+Y2c/N7FAzKzGzIVEUNRdyLqAQQginm9ntZvYLM5v20edBCKGULxnDkxDCAWa2OIqitzv++wdm9i0zuz6KogsKOVtXRwnKgxDCfWZ2rJl9wsw+bWZfM7MToyjiR+bhSgih3MzeNbMGMxvDPwSAdiGEqWZ2oZmNN7NgZlUd0eejKLq7QGN1eZSgHAshDLD2F/3FURRtF0LYycxeMrO/RFE0pbDTAfkVQjjSzB4ysxvM7CIzO8TMdjKzRjObF0XRnMJNBxRGCOEbZvY/ZrbRzO43s6VmNtzMjrb2QjQqiqJVhZuw6yop9AAOnGJmpdb+pTCLoujlEMLzZnZACGF0FEVvFnI4IM/27HjbaGYvWHsB+l8hhKfM7Lgoit7P92BAIYQQdjOzH5jZKjPbO4qixZtkH37p+Gtm9vXCTNi18Y3RObTJN0S3mdkvN4l+bu3tnm+Qhjf9O95eZO0/KLCfmXUzs13M7FEz+6SZzSjMaEBBTLP2z8Xf27QAdbjbzJqt/dspkAOUoNw60MxGmdljURQt3eTPf23tH9gnszgRznz4mtNqZkdHUfS3KIrqoih6ycyOMbP3zGxyCGFSwSYE8uvDq6OPfTSIoqjBzF4zs1EhhOq8TuUEJSi3pnW8/fmmfxhF0Rpr/76I/mb2qTzPBBTS2o63L3z4kzAfiqKo3sxmdvznxDzOBBRSj463yz4mr+t42ycPs7hDCcqREEI/M5va8Z+/2WRRYhRCiOz/Lm9O2+wBgK7ptY63az8mr+14W5n7UYCi8OHH/MCPyYd0vF2Xh1nc4Rujc+eLZlZmZs+b2fyPuc3RZnZQCGHkZr4WDHRFs6z9e4F2CCGkNrME7sNvlOb5AC+esfYviU0xs3/5QZmO3y4w1MzejKKIEpQD/Ih8joQQXjOzsWa2VxRF8z7mNleY2SVmdmUURd/O53xAoYQQHrD2fwBc0PGrAj788/+y9qWi68xsBC/68CCEsLO1/6TkMjObGEXRio4/L7H2beqHmdn5URTdULAhuzBKUA6EEPY3syfM7KUoinYRtxthZm+Z2QozGx5FUWs+5gMKKYQw1Np/JcAwa78y9IK1/5qAqdZ+lejEKIruL9iAQJ6FEM43s+vMbLW1/3RknZkdaWbjzOyPZjY1iqJM4SbsuvieoNz48Efff6pu1PGNoY+b2SAzOyrHMwFFIYqi98xsgpndZGZjzOxca/99YQ+Z2ScoQPCm44roYWb2opmdZGbnWPtPUF5gZsdQgHKHK0EAAMAlrgQBAACXKEEAAMAlShAAAHCJEgQAAFyiBAEAAJcoQQAAwCVKEAAAcIkSBAAAXMr6F6genDqe7YoJpCoqZP7G98fLfMF//0Tm5aE025H+xXZ3niHzEd+Zk+j4ST3WNiMUdIDN4DmRW6unTZJ5a5X+kDj0i7NlfvWA+TL/Se02Mr914b4yj1NfWynzsac9J/Mu+ZwI+q+04lz9MbHHif+U+ZKLxsg89fQLMt/alQz6uF9Y327xl7aV+bAf6OdUoWXznOBKEAAAcIkSBAAAXKIEAQAAlyhBAADAJUoQAABwiRIEAABcogQBAACXst4ThGTaGhtlPurCuTKfsPpcmb949k0yTwd6L7KTqqqSeRgxNNHxB/xsucx/O+Ramb/Uouc7/dd699Wc5ybKvHrxBpkPfXGBzJG9lWfrPUAvXjw90fEX3f2IzM+a+mWZRy8U9jFPVVfL/N2v7irzH51+p8xLw2Myv/zlU2Ve+cA8mRcTPiMCAACXKEEAAMAlShAAAHCJEgQAAFyiBAEAAJcoQQAAwCVKEAAAcIk9QVuZIVfNlvnuTV+V+bwLb+zMcdAV7L2LjN86X9/9tf1+2YnD/LuRfz5H5uOuqZX5iNfnJDp/W6J74z8x4KZnZL5b5kyZVxy5Uua/2EF/zF7ze71HZ+pDel/b9t99XeaZWv0x23LQBJmPufIVmX+xx29kfvkbR8q88vpeOn9069kDFIcrQQAAwCVKEAAAcIkSBAAAXKIEAQAAlyhBAADAJUoQAABwiRIEAABcYk9QFzPwer1HaO9mvd8i3bszp0E+pHYdJ/OFX+4m83sPu1nmE8tLZV6bqZf5BUsPkfnKLw2S+Xavz5d5prVV5tgKtWVk3P9m/Tpn+kPaztvhFH36mzbK/K1jb5P5yPQ0mZf1aZT5gJ5rZD7rr+Nl/u6P9HO++8pFMveEK0EAAMAlShAAAHCJEgQAAFyiBAEAAJcoQQAAwCVKEAAAcIkSBAAAXGJPkDNx+zXSO4yVud7egUJYeHa1zBcffnvMEfQeoHs29JH5ddeeIfO+t8+JOf+GmBzoXJmFek/Oxpv3kPmpF+0r87sO+anMFzQNkfk9lx8h89H3PytzdmdtOa4EAQAAlyhBAADAJUoQAABwiRIEAABcogQBAACXKEEAAMAlShAAAHCJPUFZChN2lHnvHy+T+a9HPtGZ4/ybo984VOatbXG9t06maRss86aM/pB6Z6XeOdNrVoXMu6J1J+0t88kXzpX5zAF6J0mcP2yskfllD5wg821j9wABnSs9oL/M3zxvlMz777ZS5mvr9evgsmlDZX714k/I/I3L9OeRe6/6sczPtnNk3u1e/ZqB/8OVIAAA4BIlCAAAuEQJAgAALlGCAACAS5QgAADgEiUIAAC4RAkCAAAuhSiKsrrDwanjs7uDM2u+NEnmz37/ljxNkhvr2hpk/pmpp8s8eu7lROd/rG1GSHSAHEj6nBj3vN6tdMOg55IcPtakF4+VeffDFuX0/EimKz4nQmmZzBffvb3MvzjuGZk/dNUBMu/+68Lu2UntOk7m712mH/LGBv3+G3PZBplnXt+6n/PZPCe4EgQAAFyiBAEAAJcoQQAAwCVKEAAAcIkSBAAAXKIEAQAAlyhBAADAJb2gBP+mZMhgmX/ha3/O0ySbV9/WLPOqlN4fEadHqlLmG4dX6/PnduVNUXrn8n1kfn6PO3N6/vOW7yHz5t/3jzmC3hmS7ttH5q99e0zM8Qtr+MyMzMseeTZPk+BDUYt+Hdv2S/pj8m9l+nW6e21h9wDFaXvxVZkPOUG/jr91xQSZX/XorTL/xl5HyzyzcpXMtyZcCQIAAC5RggAAgEuUIAAA4BIlCAAAuEQJAgAALlGCAACAS5QgAADgEnuCsvTaBcNl/qdehd0TtPOMc2S+6DN6P0RSbSUhp8ffGt1wkt4DdGhVU07P/8yqbWRepz+k7YSXNsr8nkXjZP7JgQv0CRK6ZNDDMh9VWiPzBcc0yPyNln4yv/rNQ2RefXUPmaef/IfM8e/aNuqPSYvLU2kZp0ePkPnSwwfIfP1Oes9RUtvdpl8zRl+v9yhNe/k8mR/4yGyZP3znvjIfeNeLMo99/PKIK0EAAMAlShAAAHCJEgQAAFyiBAEAAJcoQQAAwCVKEAAAcIkSBAAAXGJP0EeEPXeW+X2fvjHmCOWdN8xmnLJkP5lvf83b+gCf6bxZNmfjAN2r9caWrdN79+8o813L/hZzhNy+VwZVr5d5/Wq98+T3V03Rx79nrsyXyTS5k066UOaTL9TzXT1gvsx3LKuT+dRd75f5vLtaZP6ZWWfIfOxpz8nco/SA/jJv2E3vxlp2it6zM2Ov22X+sw/0npwHXhwv895zS2UeZ5ub9e6tA3oslPlt5+r3zwun7CTzS2f8UuaPflHff/F+FTJva2yUeWfiShAAAHCJEgQAAFyiBAEAAJcoQQAAwCVKEAAAcIkSBAAAXKIEAQAAl9gT9BHrvlcv8/Hlud0DVNem9yO8+80xMi9rfbczx8na+j30/HojzdZpwaR7Ym6R2z1A299xpsy3uXS2zAfa+505Tt71iNlT9Mdt9pH50JNqZX52r3eynmlTE8v1Tphb9v+VzK+3cYnOvzXa8Jm9ZX7yZQ/KfFqPR2W+qEXvfjpr6pdlvvi47jK//NgZMr/nS0NlHmd2D/0xfdv5c2T+w+H6U3/ZI6/I/KrLPxeT6z1Llz90lMxLL+0p8zD7RZlngytBAADAJUoQAABwiRIEAABcogQBAACXKEEAAMAlShAAAHCJEgQAAFxytydo1Zl6v8ILu07P0ySbN/7e82Q+6gm9E6XpwAmdOA2w9Rt6pd6T9OOeh8v8wBOuk/mOZZVZz7Spvcr1nqLBc7slOn4xajp8T5lf/P27ZT61Wu/5OXXJvjJfcpHet5Z64QWZj1rWX+aX9vq0zG958xcyj/PVeeNlvus1enfY4N+8JPO2mPP3uFt/HrrqS4fJ/IkdH5D5CddMkXndqaNlng2uBAEAAJcoQQAAwCVKEAAAcIkSBAAAXKIEAQAAlyhBAADAJUoQAABwqcvtCSoZNlTm555zX54m2bx1bQ0y7739BzL/9lvzZb5H2byYCcpi8mSGD1qT0+MDnW3bi+fI/NiGr8l84enJdov1SlfJ/K7hTyc6fiFsPHYvmf/tJ7clOv7Na4fJ/J1vjpV5+ul/JDp/ZuUqmY89U+fX27hE5x9l8xPdP24PUFLRgUtlvurdjTL/3bazZL77AWdkPdPH4UoQAABwiRIEAABcogQBAACXKEEAAMAlShAAAHCJEgQAAFyiBAEAAJe63J6gV745WOYnd9f7G3KtR6pS5vN2m5HwDLndAxTngpGPyvxm0/s7AGz9trtoQU6P/9CKXWSefiLZHiAUt39895aYW1ywxcfiShAAAHCJEgQAAFyiBAEAAJcoQQAAwCVKEAAAcIkSBAAAXKIEAQAAl7a6PUHNh+4p8wVH3xRzhMLu0QGAru7O4X8r9AjAFuFKEAAAcIkSBAAAXKIEAQAAlyhBAADAJUoQAABwiRIEAABcogQBAACXim5PUKqiQubjrnhJ5lWpwu4BWpXZKPPPnXiWzMPf5yc6/8qz95H5/G9OT3T8OCNK18i8ZMhgmbcuXdaZ4+TFIYPHy/z8N1+V+aFVTYnOv/B0/ZiOHHKazMee9lyi8yOZ5a11Mj/o2S/L/NVjOnMaILnakyfJvCrMzdMk8bgSBAAAXKIEAQAAlyhBAADAJUoQAABwiRIEAABcogQBAACXKEEAAMClotsTtPTM3WX+8JDc7rlJ6uBrL5L5wL/PztMkhbFLmd7ztH7iMJlX/X7r2xMU5+qvfkHmb1z7qMzP7vVOovOXdWuWecmwoTJvffe9ROcvdu99S+/W+s6J9yY6fm2mXuZXv7+/zIceu0CfoC3LgfLg8AOOk3nNnbUy/922s2T+yPZ/kvm2N35F5mMvfkHmUVOy3V1dXcOnJsr8wm/9WuY1Kf15YnGL3p11yhnny/ypP8r4X3AlCAAAuEQJAgAALlGCAACAS5QgAADgEiUIAAC4RAkCAAAuUYIAAIBLed8TlNppe5k/fv4PY45Q3XnD/AeuXL2dzAf++Jmcnj+93WiZX3DW73J6/jiLYvY7tE5bLfOm5j07c5yiUPbIszK/facj9AFO0TtR4vYIvbbfL2X+k0e2kfntd+n5Bv+ouHdfrZ42SeYHHztP5id1+yDR+S9f9UmZvzqhNdHxi1HmtTdlvvbiXWV+wOWfkvkTOz4g87eOv1Xmu712psz7Ty/uj+lcW//ZvWV+1DeekPkJNetkfsAC/fiWXtpT5uWz9WtqNrgSBAAAXKIEAQAAlyhBAADAJUoQAABwiRIEAABcogQBAACXKEEAAMClEEVRVnc4bPBX5R3ePHtbef+5X7hW5r3SVVnN81FPNer8uXo9X1J333SIzPvdOkfmqV3HyfyLv3tE5sfW6D08F63YS+YPzNtd5sP06a3yAb1zJanH2maEnJ7gP3Bw6vjsnkRZKhk2VOZvnDVM5pceo3dHxe3Bqc3Uy/zHa/aQ+axL95V5UoMv0jtprh/2oMz/0qD3JD1eu4PMn3xR7z7b4apVMm9drPc8xemSz4lUWsarztCvY+eefZ/MT6h5T+YT550i8/JHust8wIyFMs/U1so8TrpXL5mvPF5/TDYdul7m8ybeJfPf1enXpBt/cpzM+98Ss0+vLaPzGNk8J7gSBAAAXKIEAQAAlyhBAADAJUoQAABwiRIEAABcogQBAACXKEEAAMClrPcE7XbGdfIOfV5ukPcvWbMxq/NlbZXeeZL5YE1uzx+jdcoEmb91nN6PUbWkRObb/OF9mWdefUPmxa5L7kTJtb13kfGbZ+mPuUVT9M6QYjdqlt75MvrmmJ0kc//ZidN0Pp4T/y7dr5/MoyE6X/hVva/uK3v9VeaTqvXrbH1buczjVKWaZD5n4xiZ3/rMZJlvf5PeDRaWxnyeeV/nucaeIAAAgBiUIAAA4BIlCAAAuEQJAgAALlGCAACAS5QgAADgEiUIAAC4lPWeIAAAgK6AK0EAAMAlShAAAHCJEgQAAFyiBAEAAJcoQQAAwCVKEAAAcIkSBAAAXKIEAQAAlyhBBRBC+FwIIer432mFngcolBDCiI7nwWWFngWAP5SgPAshDDOzm8ysrtCzAADgGSUoj0IIwczuMrMPzOzWAo8DAIBrlKD8OsfMDjSzU8xsY4FnAQAUgRDCySGE+0MIb4UQGkII60MIfw8hfK7Qs3V1lKA8CSGMM7OrzOzGKIqeKvQ8AICicYuZbWNmT5nZDWb2247//lUI4YoCztXllRR6AA9CCCVm9iszW2Jm3yrwOEDRiKLobTMLhZ4DKLCdoihatOkfhBDKzOxhM/tGCOHWKIqWFma0ro0rQfnxXTPbzcxOjqKoodDDAACKx0cLUMefNZvZzdZ+sWJK3odygitBORZC2Mvar/5cG0XRnELPAwAoLiGE4Wb2dWsvO8PNrPIjNxmS96GcoATlUMeXwX5pZq+b2XcKPA4AoMiEELY1s3lm1svMnjazR81snZllzGyEmX3RzMoLNV9XRwnKrRozG9vx/xvbf0L+39wRQrjD2r9h+rx8DQYAKAoXmFkfMzsliqKfbxqEEP7b2ksQcoQSlFtNZnbnx2S7W/v3Cf3NzF4zM75UBgD+jO54e/9mssn5HMQjSlAOdXwT9GZ/LUbHrwnYzcx+EUXRT/M5FwCgaLzd8XZ/M3vowz8MIRxiH/P5A52Hnw4DAKBwpptZs5nNCCHcHUK4JoTwZ2v/8fj7Cjta10cJAgCgQKIo+qeZHWBms83sCDM7w8y6m9mnjV+vlHMhiqJCzwAAAJB3XAkCAAAuUYIAAIBLlCAAAOASJQgAALhECQIAAC5RggAAgEuUIAAA4BIlCAAAuJT17w7b6cHvyu2KLS36kCUlGZmngl7e2NhUKvPmjWUyt+aY3lfSJuNufTfKvKqsRR8/Rjqlzx9ns7+nPgs1ZU0yr0i3yrwxox//pet6yLy+rlzmi0/6VtK/Yqc7OHV8UW8cnblsfqFH2KodMnh8oUeQHmubwXNiK7P06/vI/OVzp+dpks3b6cYzZX7sZ/+a6Phzd9Wfx5PK5jnBlSAAAOASJQgAALhECQIAAC5RggAAgEuUIAAA4BIlCAAAuEQJAgAALmW9JyhuD1Ccpka9H6Blo85TG/T5K9bqXpcp1+srWnrL2OrWVer7V+n5+nTTe4aqS5tlXprWe5bKUnqPT1KrG2pkvvwDvQeoNebxtUzRrTyJNXhut0KPgBzi8UW2Dnl5vcx/8qzex/Zfx5+c6Pyrd9afp/7x3VtkfuO022S+a1nM32/NRJkXE64EAQAAlyhBAADAJUoQAABwiRIEAABcogQBAACXKEEAAMAlShAAAHAp66U/rS1pna8rk3m6Tt8/ndJ7fKwt2R6ZdJO+f0vQ50+V6DzTmqxXditrTHT/klSbzFvb9Hxxe4CWre4p80yDfnwt5uENLVvfnqC7hj9d6BGQQzy++Kg3btpL5qtu0a+DI9/Q++DC3+dnO9K/GPTWQJlvf+DnZb5w31/FnKE6y4mKF1eCAACAS5QgAADgEiUIAAC4RAkCAAAuUYIAAIBLlCAAAOASJQgAALiU9Z6gmDUvVrI+Zg9Qk75/cx99hqiiVeZt/fT9Q1rv0amq0vsbmptKZd66ukLmKyK9B2dwzTqZl6UyMu9dtlHma1sqZd4Ss0coxOxRihW354laDqDA1v/33jG30K+D/W+e3XnD/Adal6+Qecnz2+oD7Jvs/DNmTJb5MCvs+2dTfMoBAAAuUYIAAIBLlCAAAOASJQgAALhECQIAAC5RggAAgEuUIAAA4FLWe4IqK/UenboeZTLPtOo9MVGF3oMT0no/Q03Pepn3rmqQ+Yq13WSeWVMu84qVek9Sc7PeI7RugN7j062sUeZrmqv1+dv0fKUpvUeptUnfPzToPHZPUMzjCwBx0v36yXzNIaNkvnI//Xlo+1v0Pjb9Ktr1Dft+8ewBisOVIAAA4BIlCAAAuEQJAgAALlGCAACAS5QgAADgEiUIAAC4RAkCAAAuZb0naGD3DTJftEHvwbENpTpPuCamNK33O6Tj9uC0xuy5yeg9N0Gf3tKNMXtyYlSV6D1NzW36IW3M6Pf/mnq9pyju7x+V6gcwFfP3T9fRywEkE7cH6L4rfyTzo66+WOZt81/JeiYUJz7jAAAAlyhBAADAJUoQAABwiRIEAABcogQBAACXKEEAAMAlShAAAHAp6z1B9S16z0y0UR8ybk9OJqV7WRT0np90Su+pqSxpkXmv7vUyXxOzJ6eptVzmmW56kVC/yjqZD6pYL/ONMedf0lom824VTTJvrNb3b6nV5y9dpx/f8jUyBoBYc6+5VeZj/3qWzEfePLszx0ER40oQAABwiRIEAABcogQBAACXKEEAAMAlShAAAHCJEgQAAFyiBAEAAJey3hNUW1cl83RdWuYpvabHorg9QS0xe4badD6gYoPM2yJ9/za9hsjWNOu/v8XM9/b63jLvW673CFWmm2XeL+b+FWn9AJWm9J6m5anuMm9bWy3zqlUx72AA7g2dWyPzK1dvJ/NR096SuX6VK35tk3eT+ZNf/WHMEfTr9CfPnCbzSpsXc/ziwZUgAADgEiUIAAC4RAkCAAAuUYIAAIBLlCAAAOASJQgAALhECQIAAC5lvSeoYbXeE5Qq1XteWit1HqVi9sToNTtWmtYbHipj9uDE7QnaUFepB2jV9w/lyTZQNGRKZV5akpF5z9L6RHm3kiaZx1kyRH/Ird9Qkej4ALZ+b/9gkszvHKz33NxRu5fM2zbofXFbu7a0vr7RN633AMUpXa8/z2xNuBIEAABcogQBAACXKEEAAMAlShAAAHCJEgQAAFyiBAEAAJcoQQAAwKWs9wSVrk3LPFMRs+cnTsweIIvZI1TXWC7zxxePlXnm7RqZl6/RA5boNT7WMKJZ5kO7rZV5z9IGmadN7yHKBN174/Ykxako0XuYKmv0nqGmnRJ+/AAoeo1HTpT5Ocf8UeZDS/TrNLCluBIEAABcogQBAACXKEEAAMAlShAAAHCJEgQAAFyiBAEAAJcoQQAAwKWs9wRFMbUp1RSzR2etPkBo1cfPVOo9MvWtMfsj9BodK6/T86f0Ghxri9kTVFqt9wSNq1kh823KV8t8WXMvmde2Vsl8dbN+/y2v7y7ztY2VMq+v1Xn3l8pkbv+t40I4ZPD4Qo8gzVw2v9AjbNWK/fF9LOY1rRBKhg2V+YZp62R+Vs93E53/Nw9/UuYjbU6i4xe7t47X+/zinLpkX5mXL18v80yis+cXV4IAAIBLlCAAAOASJQgAALhECQIAAC5RggAAgEuUIAAA4BIlCAAAuJT1nqCeO3wg8w315TJvqtd7YKKmmF6m1/hYVZ96ffxIH6ChVO/RaV2v9y9EQxplfsJ282V+ULeXZf5+Ru/peSfqK/PSoDc4ZGLePx/UV8t8zZu9ZT7wGRlbzwfm6xtcr2MAhbf8iGEy/9p2v8vp+Ud+o2vvAYqz+FO3J7r/M7/fReZDXp2d6PhxUuN3kHn90Jh9gNmcq9OOBAAAsBWhBAEAAJcoQQAAwCVKEAAAcIkSBAAAXKIEAQAAlyhBAADApaz3BN2ww29lHrfHZl7dtjKfv3aozJev18fvXa33BPUq13nVkGaZj61epfOK5TLfs3ypzNtkaraitafMB5Suk/mixv4yX92o9y+sXqLPP/SpSOY1f35R5m2Nes8SgOJ31BlPyfx/fvYZmZ907vTOHKfLSffqVegREklVVMh8yXf0vrqZe14Xc4aLtnyWLb4lAABAF0IJAgAALlGCAACAS5QgAADgEiUIAAC4RAkCAAAuUYIAAIBLWe8JitMt1SDzIeW1Ml9e2UPmmTbd2ypLWmQ+qma1zE/qNVfmu5Tp/Qbr2vTf//H6wTJ/q7mfzHuk9fF7l9TJfGVav39LUnpTUapJv/+bq/V+h/VH7Srz1gp6OQAoez6p99UVu3fP3V3mCybpPVFTPneuzJ94fMtn4TMOAABwiRIEAABcogQBAACXKEEAAMAlShAAAHCJEgQAAFyiBAEAAJey3hO0oGmozKtTTTKvy+g9O33L9J6b9xtrZL64trfMVzdUy3xQ2VqZv9u6QuZXLzpG5ktfHSDzmhHrZH7KmDkyT5ve81PfVibztY2VMg/99eO7ap9SmVes1B9y5WtkDGAr8PgP9pP5vddcG3ME/ToUJ/2E3seWOWBZouPn2uu3TJT5/X1+HHME/Xl29+c+I/Pht78q80zM2aNPjJf5vWfox3/sLy+Q+ajZL8RMsOW4EgQAAFyiBAEAAJcoQQAAwCVKEAAAcIkSBAAAXKIEAQAAlyhBAADApaz3BPVJ6z0+G9r0foLGNr1HpiVKy3z5hm76/O91l3l9ve59dzVOknk6pffwrH+9l8xTrUHmew9+W+ZtkZ5/UWN/ma9q0u+/XXrr/RkDq9fL/JVVA2XeXKsfnyil3z8Ail/F6haZ71iWbA9QnJ9s+zuZH3XJxTIf9v3Zic6fGr+DzMP1a2V+07BfyrwmpT/PxqnbqO+fqa1NdPzRNyyU+bffmarv/4vVMs80NmY70sfiShAAAHCJEgQAAFyiBAEAAJcoQQAAwCVKEAAAcIkSBAAAXKIEAQAAl7LeE/RuS2+dN+p8aWNPmX/QWC3zNe/rPTNVS/WeoTjr1+jzDxvygcwzo9fJPG7P0MByvYend4ne01Se0vs5epVulHlFaJX5uGo9f6+yepk/lRkt8/pUjcwBIM6oUv068rkTZsn8jiGTE52/qr9+nV2w3Z9lvv/LU2VeMeohmU+pzMg8qXcv2UfmF/e+TeaXXHe6zLu/Ojfrmf5TXAkCAAAuUYIAAIBLlCAAAOASJQgAALhECQIAAC5RggAAgEuUIAAA4FLWe4Leaugn8xWNeo/Poto+Ml+zUt+/+o0ymfdcpPcjNPTRva+lqlnmg2v0HqCk+sbsARpTtkLmmYS9Nm16D1Bp0O/fEWXvy3ybijUy/2vvMTIHUPzKVuk9OV9fOV7mVw+Y33nDbMa3+r6m80/pPM6C5gaZ7/7cyTKve0Xv21s7Qu+zM9P75iZu847MF5yl9wA1j9N/v9MfPk3mY36Tvz1AcbgSBAAAXKIEAQAAlyhBAADAJUoQAABwiRIEAABcogQBAACXKEEAAMClrPcElada9QGD3jOTTkX6BG1Bxi01+v5rR6VlXj9U77kZ00/vsRlSsVbmfcv0foySlD7/sLIPZF4dWmS+PiqXeUuk3z/VKb0nqVtKn79b0O+/TKXu3QMG53YPE4Dca3t5ocxnTZ8k8wXfniPzHcsqs56pM92zQe+7u/HNA2Xe72i9h2j9tXvLfHLlcpmb6T1Cd494UuazLnha5tecdJI+/dxndF5EuBIEAABcogQBAACXKEEAAMAlShAAAHCJEgQAAFyiBAEAAJcoQQAAwKWs9wSNqFgt81EVq2S+Xc1KmdcPLpN5RcyemiFltTIfX/GOzEeU6D0572d0b1yRqZF52vSeo4qEe4DWxJw/TrdUo86Dnr9bWu8h6pnSHx8t5ToHsPXrc4feA3TMgV+R+euTf9GZ42TtZ2dMlXmvvzyf6PijvjZX5j85aKLML++3INH5z739yzIfMnd2ouMXE64EAQAAlyhBAADAJUoQAABwiRIEAABcogQBAACXKEEAAMAlShAAAHApRJHe+wIAANAVcSUIAAC4RAkCAAAuUYIAAIBLlCAAAOASJQgAALhECQIAAC5RggAAKCIhhN1DCA0hhEdCCKWFnqcrY09QDoUQhm7J7aIoei/XswAAil8Ioa+ZPWdmS83s4CiK6gs8UpdGCcqhEMIWvXOjKAq5ngUAUNxCCGkzm2lm/c1schRFtQUeqcvjy2G5N9rMSj/mfyMKNxYAoMiMM7OnzewQClB+lBR6AAcyURS1bi4IIWTyPQwAoDhFUfSymb1c6Dk84UoQgKISQhgRQohCCCcXehYgH0IIJ4cQ7g8hvNXxDdHrQwh/DyF8rtCzdXVcCQJQbBrN7K9mtqLQgwB5cou1XwF6ysyWm1kfMzvMzH4VQtguiqLvFHK4roxvjM6hjm+MHhlF0dsfkw81s3f5xmgA8CuEMDyKoiUf+bNSM3vIzKaY2YgoipYWZLguji+HASgaIYSSEMJpHV8auD2EMLbQMwG5tmkBCiF0DyEMsParQfda+1dsphRqtq6OL4cBKAohhGBmD1r7lwE+dFII4cAoip4p0FhAzoUQdjSzy8zsIDPruZmbDMnnPJ5wJQhAzoQQhocQLgshHLIFN/+ktRegJ639X8Gnm1mVmX0vdxMChRVCGGdmc629AN1hZp8zsyOt/bnwzY6blRdmuq6PK0EAculsM7vQzCZvwW0Hdrx9OIqiNWb20xDC/5jZDrkaDigC55pZjbVvh3580yCEMHDzd0Fn4UoQgFz6sMAs2ILbfnibL4QQKkMI3az9kwM/JYaubETH27mbyQ7K4xwuUYIA5NL7HW8/EXfDjkVxT5rZjmb2FzObYWYVZvanXA0HFIFFHW83/V44CyEcamafzf84vlCCAOTS7WYWmdmMEMLVIYTxIYReHT8FFjqu+PQNIWzf8aI/p+N+e5vZh99HdGQI4ckQwm8L8RcAcmy6mTWZ2T0hhN+EEK4MIfzB2sv/rws6mQN8TxCAnImiaHYI4Tgzu83MLu7435ZYYO1fBtvdzCZ0/Nk7nT8hUFhRFC0IIUw2syutvfiXmtmLZvZpM1tnZicVcLwuj2WJOcSyRKBdCKHSzI41s73MbLCZdbP2f4Q1mVmdma0xs7es/UsDz0ZRROEBkHOUoByiBAEAULwoQTnUUYJiUYIAAMg/vicot4YVegAAALB5XAkCAAAu8SPyAADAJUoQAABwiRIEAABcogQBAACXsv7psLYVYxJ9J3VdW6PMjx26d5LD51x6QH+Z//mFRxMdf/Q9Z8h81EVzZF5or/90D5kvPvyniY6fGvhG0a0TODh1PD9d0IXNXDY/0f2P2OtImbe++16i4z/WNoPnBDpVKC+X+fIzJ8j8xYumJzp/XE+YOPc0mS/89KVb/JzgShAAAHCJEgQAAFyiBAEAAJcoQQAAwCVKEAAAcIkSBAAAXKIEAQAAl/gt8gAA4H8tuUjvAXrlTL0H6L3WOpnvN/N8mQ+Zqa/PDLvvGZlbm443xZUgAADgEiUIAAC4RAkCAAAuUYIAAIBLlCAAAOASJQgAALhECQIAAC6xJyhbzS0yPm/5HokOX7U8JLo/gM51xF5HJrp/63tLO2kSYMuUDBwg81cvGSHztz6t9wDds6GPzH96zmkyHzvzWZnnE1eCAACAS5QgAADgEiUIAAC4RAkCAAAuUYIAAIBLlCAAAOASJQgAALjEnqAsZWprZf7qhGTHH2Szkx0AQKdqffe9Qo8AZOWVK4bLfNexb8t85MxTZb7Dd1fKvOzd52ReTLgSBAAAXKIEAQAAlyhBAADAJUoQAABwiRIEAABcogQBAACXKEEAAMAl9gQBgNB41ESZr95Zv4wOv/FFmbdt3Jj1TIAy9vRnZd4Qd3/Te4Bas5ynmHElCAAAuEQJAgAALlGCAACAS5QgAADgEiUIAAC4RAkCAAAuUYIAAIBL7AkC4FrtyZNkPu/KWxIdf/UZeg/QiZ8/W+bpJ/+R6PzIXmrXcTJfckQvmV/zpZ915jhdzhWXnSzzHnfPzc8gxpUgAADgFCUIAAC4RAkCAAAuUYIAAIBLlCAAAOASJQgAALhECQIAAC6xJwhAl9a2324yf+B7P4w5Qk2i8/dNV8u85xVLZF5/TJ9E58e/CyX6U9+v/3inzHulq2Re19aY9UzFpDSkZV4eShMdf9crfyTzU+/eN9Hxs8GVIAAA4BIlCAAAuEQJAgAALlGCAACAS5QgAADgEiUIAAC4RAkCAAAusScoS+levWQ+9vENiY7/l3smynzQdbMTHR/ockKQ8bpv1Ml8UIneAzT2qS/IfMjPy2V+/S03yfy+UY/LfMoup8oc2Vv30DYyj9sDFOfYoXsnun+hrZ42SebPX3ZLouN/6vsXybyvzUl0/GxwJQgAALhECQIAAC5RggAAgEuUIAAA4BIlCAAAuEQJAgAALlGCAACAS+wJylZZqYxvGPRcosOPHrRnovsD3pQMHSLzebvNkPmshrTMR574z6xn2tTnp58v85fOny7zt09pS3R+j8IeO8m8T+UHMr957TCZ/+jJw2Q+1ubJvNjt8qWXE93/6yvHy3zA3/T7P5Po7NnhShAAAHCJEgQAAFyiBAEAAJcoQQAAwCVKEAAAcIkSBAAAXKIEAQAAl9gTBMC1abO/IPPR9kKi4w//7RJ9A71GCP+B6Dm956Zlf33/B62PzLf2PUBxe5TuGn53ouPPeHovmY955ZlEx+9MXAkCAAAuUYIAAIBLlCAAAOASJQgAALhECQIAAC5RggAAgEuUIAAA4BJ7goCEVn95kszX7tso8+3OeVvmmdrabEdCFs7a7a8yn2ndEx2/bY1+/C5/fweZL5pyV8wZvpXlRPBu3RUNOT3+mHOKZw9QHK4EAQAAlyhBAADAJUoQAABwiRIEAABcogQBAACXKEEAAMAlShAAAHCJPUFAQtdcfLvM735/H5kvr2/uzHHcyax6X+anLNlP5ncNf1rmf5h5jMw33j9Q5msmtMr88u7TZW5WGpPjo16fPlHmF+7/cJ4mKU6HV/8t5hY1Mh13+5kyH26zs5yocLgSBAAAXKIEAQAAlyhBAADAJUoQAABwiRIEAABcogQBAACXKEEAAMAl9gQBCU2pzMj8ktr+Mu/etKgzx3EnamqS+dyHJ8i8adpfZP7Uzr/XA+ys43jsAfqosNuOMh90yxKZzxyud3dB7wH6+Xr9mjV0VkNnDlNQXAkCAAAuUYIAAIBLlCAAAOASJQgAALhECQIAAC5RggAAgEuUIAAA4BJ7ggB0acMvny3zXUrOkfn0z+qdM3F7ouraGmX+Skta5hPL/e0RWrFvD5k/MvxpmTdFLTLf53L9mPf7xT9kXmipmmqZ//klvfsqznULp8h80NMvJDp+MeFKEAAAcIkSBAAAXKIEAQAAlyhBAADAJUoQAABwiRIEAABcogQBAACXst4T9OX3Jsn8tqFz/uNhtgYf/NeoQo9Q1E6Y8FyhRwCyMuI7+jXrup8dKfNrBuidNqFF7xFa9329R2jOrvfLvCua/83pie6//z9PlHnf2/VjHiU6e+7V/mFoTo8/aOqrOT1+MeFKEAAAcIkSBAAAXKIEAQAAlyhBAADAJUoQAABwiRIEAABcogQBAACXst4T9Nir4/QNtvI9QWHCjjKf8YMfxhyhJtH5B++yItH9c23Rr8fLfOaAn+dljmKyoLlB5nF7Xg6dcJLMo+cXZD0TOk/r4nf0DRbHHKBEv8wOqu6V3UBdwJLv7hNzi/mJjl9+U++YWyxKdPxcaz5kD5lfMubuRMcfOfNUmY+15xMdf2vClSAAAOASJQgAALhECQIAAC5RggAAgEuUIAAA4BIlCAAAuEQJAgAALmW9J6h8UYW+wcE6rknp+795/d4yH33+XH2CGK1TJsh80rXzZN47pd9lD26skvnR1fUyn7XTfTLf+fKzZb7NpbNlnurWTeZvfnsnmf/zkzfK3KwsJu96jn76TJkvmnKXzKf+6kmZPzhVPycyrxf3zhPvVpwxUebzR0/P0yT5UzJsqMz3PvylRMe/oXaEzMv/9Gyi4xfaksP055kjqhplPq+pRebDHkhnPVNXxZUgAADgEiUIAAC4RAkCAAAuUYIAAIBLlCAAAOASJQgAALhECQIAAC5lvSdo5L2rZP76qRtlPra0WuZzjrtW5ld+crLM43y2960yn1heKvNt7ztX5lF5m8yPPuoOmZcGvb/h4ZOvkfmNR+4v8x4la/Tx+90i87mNer7zFh6n7z9e70HaGm13ZZ3M/7B3jcy/0nOpzA9+7Bcyv3TpkTKf88z2MocW9dI7V347Wb+mTCj7R8wZ9L9FR806ReaLT4o5fAGsOGyYzP80/I+Jjv+rGw+TeV+bk+j46THbynzV5AGJjh9n0Qn6dTjOuQtPlHn3P+h9eJ5wJQgAALhECQIAAC5RggAAgEuUIAAA4BIlCAAAuEQJAgAALlGCAACASyGKoqzucHDqeHmHpd/YR97/+bNvlHl50Ht6cu24RQfJfOMh9TJP9egu80kz35b5JX0XyjzXajP673fYNy6Q+coDWmW++PCfZj3TplID3wiJDpADcc+JZRfq58QVX/6lzKdW6z1E2LrFvebUH6Nfox9ZdWvRPSeefnuUHPoTFcn+/f33Rr2P7e2WfomOP6L0fZknnT/XjtjnaJm3vr0kT5MUxmNtM7b4OVHcjyQAAECOUIIAAIBLlCAAAOASJQgAALhECQIAAC5RggAAgEuUIAAA4FKn7wmKs+xivTPlpfOmJzm8rWtrkPled35N5tv+arnMM28uznqmfzFxZxkvOqFa5m9+9tZEp3+9ZaPMP/v9C2Xe5445Mm/41ESZr9q9ROZxXv/O+UW3EyXpc6Jk2FCZLzptuMz3O/RFmd8x7O9Zz4Qtd8CCT8l8zZ+GyHzI3a/JPLP6A5lnsxMlXybPulA+J57Y8YF8jfIfGfngtET3HzFqpcyT/v3jPubKDn4n0fG3duwJAgAAiEEJAgAALlGCAACAS5QgAADgEiUIAAC4RAkCAAAuUYIAAIBLWe8JAgAA6Aq4EgQAAFyiBAEAAJcoQQAAwCVKEAAAcIkSBAAAXKIEAQAAlyhBAADAJUoQAABwiRIEAABcogQBAACXKEEAAMCl/w/bKOCElAXmHwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 720x720 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#-- Plot the data\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(6,6), dpi=120)\n",
    "\n",
    "for n in range(9):\n",
    "  plt.subplot(3,3,n+1)\n",
    "  plt.imshow(features['image'][..., n])\n",
    "  plt.title(chr(features['m_label'][n]))\n",
    "  plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '', '', '', '', '', '', '', '', '']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-- Decodes string into a list of columns\n",
    "text = pathlib.Path(titanic_file_path).read_text()\n",
    "lines = text.split('\\n')[1:-1]\n",
    "\n",
    "all_strings = [str()]*10\n",
    "all_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type: string, shape: (627,)\n",
      "type: string, shape: (627,)\n",
      "type: string, shape: (627,)\n",
      "type: string, shape: (627,)\n",
      "type: string, shape: (627,)\n",
      "type: string, shape: (627,)\n",
      "type: string, shape: (627,)\n",
      "type: string, shape: (627,)\n",
      "type: string, shape: (627,)\n",
      "type: string, shape: (627,)\n"
     ]
    }
   ],
   "source": [
    "features = tf.io.decode_csv(lines, record_defaults=all_strings) \n",
    "\n",
    "for f in features:\n",
    "  print(f\"type: {f.dtype.name}, shape: {f.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,male,22.0,1,0,7.25,Third,unknown,Southampton,n\n"
     ]
    }
   ],
   "source": [
    "print(lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, '', 0.0, 0, 0, 0.0, '', '', '', '']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_types = [int(), str(), float(), int(), int(), float(), str(), str(), str(), str()]\n",
    "titanic_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type: int32, shape: (627,)\n",
      "type: string, shape: (627,)\n",
      "type: float32, shape: (627,)\n",
      "type: int32, shape: (627,)\n",
      "type: int32, shape: (627,)\n",
      "type: float32, shape: (627,)\n",
      "type: string, shape: (627,)\n",
      "type: string, shape: (627,)\n",
      "type: string, shape: (627,)\n",
      "type: string, shape: (627,)\n"
     ]
    }
   ],
   "source": [
    "#-- description of storage\n",
    "features = tf.io.decode_csv(lines, record_defaults=titanic_types) \n",
    "\n",
    "for f in features:\n",
    "  print(f\"type: {f.dtype.name}, shape: {f.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, b'male', 22.0, 1, 0, 7.25, b'Third', b'unknown', b'Southampton', b'n']\n"
     ]
    }
   ],
   "source": [
    "#-- Does the same as decode_csv\n",
    "simple_titanic = tf.data.experimental.CsvDataset(titanic_file_path, record_defaults=titanic_types, header=True)\n",
    "\n",
    "for example in simple_titanic.take(1):\n",
    "  print([e.numpy() for e in example])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, b'male', 22.0, 1, 0, 7.25, b'Third', b'unknown', b'Southampton', b'n']\n"
     ]
    }
   ],
   "source": [
    "#-- This is the same as above\n",
    "def decode_titanic_line(line):\n",
    "  return tf.io.decode_csv(line, titanic_types)\n",
    "\n",
    "manual_titanic = (\n",
    "    # Load the lines of text\n",
    "    tf.data.TextLineDataset(titanic_file_path)\n",
    "    # Skip the header row.\n",
    "    .skip(1)\n",
    "    # Decode the line.\n",
    "    .map(decode_titanic_line)\n",
    ")\n",
    "\n",
    "for example in manual_titanic.take(1):\n",
    "  print([e.numpy() for e in example])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AGENCY,AGENCY FB,64258,0.400000,0,0.000000,35,21,51,22,20,20,1,1,1,21,101,210,255,255,255,255,255,255,255,255,255,255,255,255,255,255,1,1,1,93,255,255,255,176,146,146,146,146,146,146,146,146,216,255,255,255,1,1,1,93,255,255,255,70,1,1,1,1,1,1,1,1,163,255,255,255,1,1,1,93,255,255,255,70,1,1,1,1,1,1,1,1,163,255,255,255,1,1,1,93,255,255,255,70,1,1,1,1,1,1,1,1,163,255,255,255,1,1,1,93,255,255,255,70,1,1,1,1,1,1,1,1,163,255,255,255,1,1,1,93,255,255,255,70,1,1,1,1,1,1,1,1,163,255,255,255,141,141,141,182,255,255,255,172,141,141,141,115,1,1,1,1,163,255,255,255,255,255,255,255,255,255,255,255,255,255,255,209,1,1,1,1,163,255,255,255,6,6,6,96,255,255,255,74,6,6,6,5,1,1,1,1,163,255,255,255,1,1,1,93,255,255,255,70,1,1,1,1,1,1,1,1,163,255,255,255,1,1,1,93,255,255,255,70,1,1,1,1,1,1,1,1,163,255,255,255,1,1,1,93,255,255,255,70,1,1,1,1,1,1,1,1,163,255,255,255,1,1,1,93,255,255,255,70,1,1,1,1,1,1,1,1,163,255,255,255,1,1,1,93,255,255,255,70,1,1,1,1,1,1,1,1,163,255,255,255,1,1,1,93,255,255,255,70,1,1,1,1,1,1,1,1,163,255,255,255,1,1,1,93,255,255,255,70,1,1,1,1,1,1,1,1,163,255,255,255,1,1,1,93,255,255,255,70,1,1,1,1,1,1,1,1,163,255,255,255,1,1,1,93,255,255,255,70,1,1,1,1,1,1,1,1,163,255,255,255,1,1,1,93,255,255,255,70,1,1,1,1,1,1,1,1,163,255,255,255\n"
     ]
    }
   ],
   "source": [
    "#-- Figure out the dataset types in order to parse\n",
    "font_line = pathlib.Path(font_csvs[0]).read_text().splitlines()[1]\n",
    "print(font_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- By counting the columns we can figure out the columns\n",
    "num_font_features = font_line.count(',')+1\n",
    "font_column_types = [str(), str()] + [float()]*(num_font_features-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fonts\\\\AGENCY.csv'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "font_csvs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Wehn passing a list of files AGENCY.csv is read first\n",
    "simple_font_ds = tf.data.experimental.CsvDataset(\n",
    "    font_csvs, \n",
    "    record_defaults=font_column_types, \n",
    "    header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'AGENCY'\n",
      "b'AGENCY'\n",
      "b'AGENCY'\n",
      "b'AGENCY'\n",
      "b'AGENCY'\n",
      "b'AGENCY'\n",
      "b'AGENCY'\n",
      "b'AGENCY'\n",
      "b'AGENCY'\n",
      "b'AGENCY'\n"
     ]
    }
   ],
   "source": [
    "for row in simple_font_ds.take(10):\n",
    "  print(row[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "font_files = tf.data.Dataset.list_files(\"fonts/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "     b'fonts\\\\SERIF.csv'\n",
      "     b'fonts\\\\VINER.csv'\n",
      "     b'fonts\\\\AGENCY.csv'\n",
      "     b'fonts\\\\MAGNETO.csv'\n",
      "     b'fonts\\\\PANROMAN.csv'\n",
      "    ...\n",
      "\n",
      "Epoch 2:\n",
      "     b'fonts\\\\BANKGOTHIC.csv'\n",
      "     b'fonts\\\\SIMPLEX.csv'\n",
      "     b'fonts\\\\CALIFORNIAN.csv'\n",
      "     b'fonts\\\\MONOTXT.csv'\n",
      "     b'fonts\\\\FRANKLIN.csv'\n",
      "    ...\n"
     ]
    }
   ],
   "source": [
    "#-- Shuffle file name\n",
    "print('Epoch 1:')\n",
    "for f in list(font_files)[:5]:\n",
    "  print(\"    \", f.numpy())\n",
    "print('    ...')\n",
    "print()\n",
    "\n",
    "print('Epoch 2:')\n",
    "for f in list(font_files)[:5]:\n",
    "  print(\"    \", f.numpy())\n",
    "print('    ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Create map function to create child dataset of each column\n",
    "def make_font_csv_ds(path):\n",
    "  return tf.data.experimental.CsvDataset(\n",
    "    path, \n",
    "    record_defaults=font_column_types, \n",
    "    header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "font_rows = font_files.interleave(make_font_csv_ds,\n",
    "                                  cycle_length=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mauri\\AppData\\Local\\Temp\\ipykernel_5836\\998453860.py:5: DeprecationWarning: an integer is required (got type numpy.float32).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  fonts_dict['character'].append(chr(row[2].numpy()))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>font_name</th>\n",
       "      <th>character</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EUROROMAN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COUNTRYBLUEPRINT</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PMINGLIU-EXTB</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EUROROMAN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>COUNTRYBLUEPRINT</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PMINGLIU-EXTB</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>EUROROMAN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>COUNTRYBLUEPRINT</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PMINGLIU-EXTB</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>EUROROMAN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          font_name character\n",
       "0         EUROROMAN         \n",
       "1  COUNTRYBLUEPRINT         \n",
       "2     PMINGLIU-EXTB         \n",
       "3         EUROROMAN         \n",
       "4  COUNTRYBLUEPRINT         \n",
       "5     PMINGLIU-EXTB         \n",
       "6         EUROROMAN         \n",
       "7  COUNTRYBLUEPRINT         \n",
       "8     PMINGLIU-EXTB         \n",
       "9         EUROROMAN         "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fonts_dict = {'font_name':[], 'character':[]}\n",
    "\n",
    "for row in font_rows.take(10):\n",
    "  fonts_dict['font_name'].append(row[0].numpy().decode())\n",
    "  fonts_dict['character'].append(chr(row[2].numpy()))\n",
    "\n",
    "pd.DataFrame(fonts_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Cacching helps load faster all the csv files\n",
    "\n",
    "BATCH_SIZE=2048\n",
    "fonts_ds = tf.data.experimental.make_csv_dataset(\n",
    "    file_pattern = \"fonts/*.csv\",\n",
    "    batch_size=BATCH_SIZE, num_epochs=1,\n",
    "    num_parallel_reads=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n",
      "....................\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "for i,batch in enumerate(fonts_ds.take(20)):\n",
    "  print('.',end='')\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- This will run in less than a sec\n",
    "fonts_files = tf.data.Dataset.list_files(\"fonts/*.csv\")\n",
    "fonts_lines = fonts_files.interleave(\n",
    "    lambda fname:tf.data.TextLineDataset(fname).skip(1), \n",
    "    cycle_length=100).batch(BATCH_SIZE)\n",
    "\n",
    "fonts_fast = fonts_lines.map(lambda x: tf.io.decode_csv(x, record_defaults=font_column_types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n",
      "....................\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "for i,batch in enumerate(fonts_fast.take(20)):\n",
    "  print('.',end='')\n",
    "\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "47de56ac160514ce2b0c9ebfdfcb94cee65a80650c9f1df16b0cb8dc9368e304"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
